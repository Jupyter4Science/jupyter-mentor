[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jupyter-mentor",
    "section": "",
    "text": "Assuming you have conda installed…\n\nCreate a new environment using version 3.11 of python with the following command:\n\nconda create -f environment.yml\n\nGo to the directory of your cloned repository and activate your envrionment:\n\nconda activate jupyter-mentor\n\nOpen the JupyterLab editor:\n\ncd path/to/your/cloned/repo\njupyter lab",
    "crumbs": [
      "jupyter-mentor"
    ]
  },
  {
    "objectID": "index.html#developing-locally-with-conda",
    "href": "index.html#developing-locally-with-conda",
    "title": "jupyter-mentor",
    "section": "",
    "text": "Assuming you have conda installed…\n\nCreate a new environment using version 3.11 of python with the following command:\n\nconda create -f environment.yml\n\nGo to the directory of your cloned repository and activate your envrionment:\n\nconda activate jupyter-mentor\n\nOpen the JupyterLab editor:\n\ncd path/to/your/cloned/repo\njupyter lab",
    "crumbs": [
      "jupyter-mentor"
    ]
  },
  {
    "objectID": "index.html#developing-with-docker",
    "href": "index.html#developing-with-docker",
    "title": "jupyter-mentor",
    "section": "Developing with Docker",
    "text": "Developing with Docker\nWARNING! Development with Docker is not recommended at this time. However, GitHub actions runs repo2docker to verify that this repository is compatible with Binder.\n\nDeveloping on MacOS\n\nInstall Docker with brew install --cask docker. If this command does not work, you may need to install Homebrew\nStart Docker by clicking on application icon or by running open -a Docker\nPull the docker image from DockerHub to you local machine using docker image pull brewer36/jupyter-mentor:macos\ncd &lt;location_of_your_clone&gt;\nLaunch JupyterLab with docker run -it --rm -p 8888:8888 brewer36/jupyter-mentor:macos jupyter lab --ip=0.0.0.0 --port=8888\n\n\n\nUpdating brewer36/jupyter-mentor:latest (linux) image with GitHub acitons\nNo extra steps required. This GitHub action automatically updates the image with each new commit.\n\n\nUpdating brewer36/jupyter-mentor:macos (arm64 for M1 chip) image with repo2docker\n\nInstall repo2docker with python3 -m pip install jupyter-repo2docker\ncd &lt;location_of_your_clone&gt;\nrepo2docker --Repo2Docker.platform=linux/arm64 --image-name=brewer36/jupyter-mentor:macos ./",
    "crumbs": [
      "jupyter-mentor"
    ]
  },
  {
    "objectID": "01_tutorial.html",
    "href": "01_tutorial.html",
    "title": "JupyterAI 101",
    "section": "",
    "text": "Read this short tutorial to learn more about JupyterAI and how to use it at its current state.\n\n\nWhat is JupyterAI?/Background\n\nIntro\nJupyterAI was developed by parent company JupyterLab (which is the hub of all sorts of Jupyter projects i.e. JupyterNotebooks, JupyterLab, etc.) in August 2023. JupyterAI is a generative AI extension for JupyterLab by assisting users through an interactive chat box. Similar to how ChatGPT works, JupyterAI has a chatbox or “conversational assistant” on the left sidebar which is specifically for Jupyter and software-related use cases, therefore the answers are more pertinent and accurate. On a high level, this generative AI tool can help developers create notebooks for them, solve bugs, and perform other code-related functionalites. However, because its capablities are so expansive, JupyterAI can sometimes fall short for students who are learning how to code or getting familiarized with their environement as it provides too much information to the user too quickly!\n\n\nDefining Terminology\nFor those of you who are new to JupyterAI and AI in general, let’s first define a couple key terms:\n\nLarge language models: If you hadn’t noticed already, when you first set up your AI-chat functionality on the sidebar, you are prompted to select a lanuage model, along with an embedding model and API key. A Large Language Model is an existing pretrained dataset that acts as a middle machine to take new input from a user and convert into something usable for the user based on what it already knows. For our purposes, we will be using the OpenAI::gpt-3.5 model because…\nMagic Commands: If users don’t want to use the chatbox and instead want to interact with JupyterAI within the notebook itself, users are able to invoke magic commands (called using %ai). After calling the command, you can specify which model and/or version you want your chosen LLM to be as well as the format. For example: (give example). We are going to be building our own magic commands to activate the “jupyter-mentor” add-on to only interact with the user in a specific way.\nRetrieval-augmented generation (RAG): Think of RAG as an SSD that is stored in the LLMs so that when answering a user’s prompt, they reference their earlier conversations with you to check for data that they use to answer your current prompt. This makes their answer more curated towards the user and more accurate without having to prompt the user again for more clarification.\n\n\n\nWhy JupyterAI?\nThe JupyterNotebooks IDE is a quite popular platform for many researchers, research software-engineers, and students who use it particularly for purposes like data analysis, submitting homeworks because it is…\nDesign principles from the web\n\nVendor-agnostic. Users can choose any model\nCurrently is explicit prompt-base. Does not read/watch files are send prompts unsolicited\nTransparent: chat interface and magic commands using items that are open source so user know what gets sent to lm\ntraceable. \"users know when it has been used to generate content. When Jupyter AI generates a notebook, the notebook says that it was generated by Jupyter AI. When a user runs a Jupyter AI magic command in a notebook, output cells say, in their metadata, that they were generated by Jupyter AI\n\n\n\nWhy JupyterAI over other LLMs like ChatGPT?\nThe biggest advantage that JupyterAI provides is the ability to be use within the Jupyter Notebook environment. It can perform a lot of functionalities which are directly embedded within the platform, removing the need to ask a third-party source externally. Jupyternaut – the coined nickname for JupyterAI chat assistant – has a chatbox like ChatGPT where the human and machine can interact with one another. Jupyternaut also interacts with files in JupyterLab and can answer questions as a general-purpose AI assistant. It can only see and read the information when you specifcally ask it to by sending chat commands.\nHere are a few functionalities that Jupyternaut – the coined nickname for JupyterAI chat assistant – provides:\n- Gives a description of what a function does if you highlight the function within a code cell - Create a new notebook from a prompt - Writes AI-generated code within the notebook that answer a user’s code request - Interpolate iPython expressions which allow users to run prompts that include variable values - Learns from and ask questions about your local files - Create images - special In and Out variables which contains the inputs and outputs of code cells - Err variable which will explain the error you got when you input “{Err[line #]}”\nAdditionally, with the %ai magic command, you can select which library/model you want to render from and have it perform the output within the code cell, i.e.: - “Generate the 2D heat equation” - “Create a square using SVG with a black border and white fill”\nUltimately, any question you may have a knee-jerk reaction to ask ChatGPT to process, JupyterAI can do the same things with the added benefit of actually executing those tasks instead of merely telling you how to do those actions since it has access to your Jupyter notebook environment.\n\n\nTry it out!\nAnswer the following questions with the help of Jupyter-AI\n\n\nExercise 1: Ask Jupyternaut to create a new notebook that calculates the volume of a triangular prisim\n\n\nExercise 2: Highlight one of the functions and ask Jupyternaut what it does\n\n\nExercise 3: Use the Err call to see what the error is in this line\n\n\n\nJupyter-Mentor!\nOur use case for this project is to develop a more refined version of the existing Jupyter-AI to specfically tailor the chatbox to be trained as if it were a tutor or mentor. When prompted with a question, we expect Jupyter-mentor to not answer the question directly but rather give the user a step-by-step instructions without actually imemdiately giving away the answer. This would encourage the user to develop their critical-thinking skills while not having to figure out the hassle of directing the AI model with how they want to be given the answer.",
    "crumbs": [
      "**JupyterAI 101**"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "intro_to_langchain.html",
    "href": "intro_to_langchain.html",
    "title": "jupyter-mentor",
    "section": "",
    "text": "import langchain\n\nfrom langchain_openai import ChatOpenAI\n\n\n\nimport os\n\nwith open('OPENAI_API_KEY', 'r') as file:\n    env_value = file.read().strip()\n\n\nos.environ['OPENAI_API_KEY'] = env_value\n\n\nllm = ChatOpenAI()\n\n\nmodel = ChatOpenAI(model_name=\"gpt-3.5\")\n\n\n# llm.invoke(\"hi there!\")",
    "crumbs": [
      "intro_to_langchain.html"
    ]
  }
]